{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 5. Подмена лица"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте файл requirements.txt, чтобы установить все необходимые библиотеки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо использовать две видеозаписи на каждой из которых присутсвует человек, чье лицо необходимо заменять. Подмена работает в обе стороны.\n",
    "На первом этапе вытащим по 500 лиц из каждой видеозаписи.\n",
    "Убедитесь, что все лица вырезаны корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(num_faces=500, save_faces_path_a='train/face_A', save_faces_path_b='train/face_B', video_path_a='video/putin.mp4', video_path_b='video/trump.mp4')\n",
      "faces_process:   8%|██                         | 39/500 [01:20<03:32,  2.17it/s]"
     ]
    }
   ],
   "source": [
    "!python crop_from_video.py --save_faces_path_a train/face_A --save_faces_path_b train/face_B --video_path_a video/putin.mp4 --video_path_b video/trump.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import Autoencoder\n",
    "import torchsummary\n",
    "\n",
    "model = Autoencoder()\n",
    "torchsummary.summary(model.encoder, (3, 64, 64), batch_size=64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(model.decoder_A, (512, 8, 8), batch_size=64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchsummary.summary(model.decoder_B, (512, 8, 8), batch_size=64, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "\n",
    "ia.seed(1)\n",
    "# Аугментации данных. Позволяют разнообразить имеющийся набор данных.\n",
    "augment = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontal flips\n",
    "    iaa.Affine(\n",
    "        scale=(0.95, 1.05),\n",
    "        translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
    "        rotate=(-10, 10),\n",
    "#         shear=(-2, 2)\n",
    "    )\n",
    "], random_order=True)\n",
    "\n",
    "# Изменения входного изображения\n",
    "wrap = iaa.PiecewiseAffine(scale=0.025, nb_rows=5, nb_cols=5, )\n",
    "\n",
    "# Изменение размера изображения\n",
    "resize = iaa.Resize({\"height\": 64, \"width\": 64}, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.cvtColor(cv2.resize(cv2.imread('train/face_B/2.bmp'), (256,256)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img_aug = augment(image=image)\n",
    "\n",
    "img_input = wrap(image = img_aug)\n",
    "\n",
    "crop_percent = 0.13\n",
    "img_target = img_aug[int(crop_percent * 256):-int(crop_percent * 256), int(crop_percent * 256):-int(crop_percent * 256)]\n",
    "img_input = img_input[int(crop_percent * 256):-int(crop_percent * 256), int(crop_percent * 256):-int(crop_percent * 256)]\n",
    "\n",
    "img_input = resize(image=img_input)\n",
    "img_target = resize(image=img_target)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(131)\n",
    "plt.imshow(image)\n",
    "plt.subplot(132)\n",
    "plt.imshow(img_input)\n",
    "plt.subplot(133)\n",
    "plt.imshow(img_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generator(path, num_images=10000, batch=16, transform=None, img_size=(64, 64)):\n",
    "\n",
    "    pathes = [x.path for x in os.scandir(path) if x.name.endswith(\".jpg\") \n",
    "                 or x.name.endswith(\".png\") \n",
    "                 or x.name.endswith(\".bmp\")\n",
    "                 or x.name.endswith(\".JPG\")]\n",
    "\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(pathes)\n",
    "    \n",
    "    images = np.zeros((len(pathes), 256, 256, 3), dtype=float)\n",
    "    for i, pth in enumerate(pathes):\n",
    "        image = cv2.resize(cv2.cvtColor(cv2.imread(pth), cv2.COLOR_BGR2RGB), (256, 256)) / 255\n",
    "        images[i] = image    \n",
    "\n",
    "    input_images = np.zeros((num_images, *img_size, 3), dtype=float)\n",
    "    target_images = np.zeros((num_images, *img_size, 3), dtype=float)\n",
    "    \n",
    "    indexes = np.random.randint(0, high=len(images), size=num_images)\n",
    "    for i, ind in tqdm(enumerate(indexes), total=num_images):\n",
    "        target_image = images[ind]\n",
    "\n",
    "        if augment:\n",
    "            target_image = augment(image=target_image)\n",
    "\n",
    "        input_image = target_image.copy()\n",
    "\n",
    "        input_image = wrap(image=input_image)\n",
    "        crop_percent = 0.13\n",
    "        target_image = target_image[int(crop_percent * 256):-int(crop_percent * 256), int(crop_percent * 256):-int(crop_percent * 256), :]\n",
    "        input_image = input_image[int(crop_percent * 256):-int(crop_percent * 256), int(crop_percent * 256):-int(crop_percent * 256), :]\n",
    "\n",
    "        input_images[i] = resize(image=input_image)\n",
    "        target_images[i] = resize(image=target_image)\n",
    "        \n",
    "    while True:\n",
    "        indexes = np.random.randint(0, high=len(input_images), size=batch)\n",
    "        yield torch.tensor(input_images[indexes].transpose((0,3,1,2))), torch.tensor(target_images[indexes].transpose((0,3,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_A = generator(\"train/face_A\", num_images=5000, batch = 64)\n",
    "loader_B = generator(\"train/face_B\", num_images=5000, batch = 64)\n",
    "test_loader_A = generator(\"train/face_A\", num_images=64, batch = 64)\n",
    "test_loader_B = generator(\"train/face_B\", num_images=64, batch = 64)\n",
    "\n",
    "inp, trg = next(loader_A)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "i = 0\n",
    "plt.subplot(241)\n",
    "plt.imshow(inp[i].numpy().transpose((1,2,0)))\n",
    "plt.subplot(242)\n",
    "plt.imshow(trg[i].numpy().transpose(1,2,0))\n",
    "\n",
    "inp, trg = next(test_loader_A)\n",
    "\n",
    "plt.subplot(243)\n",
    "plt.imshow(inp[i].numpy().transpose((1,2,0)))\n",
    "plt.subplot(244)\n",
    "plt.imshow(trg[i].numpy().transpose(1,2,0))\n",
    "\n",
    "inp, trg = next(loader_B)\n",
    "\n",
    "plt.subplot(245)\n",
    "plt.imshow(inp[i].numpy().transpose((1,2,0)))\n",
    "plt.subplot(246)\n",
    "plt.imshow(trg[i].numpy().transpose(1,2,0))\n",
    "\n",
    "inp, trg = next(test_loader_B)\n",
    "\n",
    "plt.subplot(247)\n",
    "plt.imshow(inp[i].numpy().transpose((1,2,0)))\n",
    "plt.subplot(248)\n",
    "plt.imshow(trg[i].numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "print('===> Try resume from checkpoint')\n",
    "if os.path.isdir('checkpoint'):\n",
    "    try:\n",
    "        checkpoint = torch.load('./checkpoint/autoencoder.t7')\n",
    "        model.load_state_dict(checkpoint['state'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print('===> Load last checkpoint data')\n",
    "    except FileNotFoundError:\n",
    "        print('Can\\'t found autoencoder.t7')\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    print('===> Start from scratch')\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer_1 = optim.Adam([{'params': model.encoder.parameters()},\n",
    "                          {'params': model.decoder_A.parameters()}]\n",
    "                         , lr=1e-4, betas=(0.5, 0.999))\n",
    "optimizer_2 = optim.Adam([{'params': model.encoder.parameters()},\n",
    "                          {'params': model.decoder_B.parameters()}]\n",
    "                         , lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training')\n",
    "for epoch in range(start_epoch, 5000):\n",
    "\n",
    "    input_A, target_A = next(loader_A)\n",
    "    input_B, target_B = next(loader_B)\n",
    "\n",
    "    input_A, target_A = input_A.to(device).float(), target_A.to(device).float()\n",
    "    input_B, target_B = input_B.to(device).float(), target_B.to(device).float()\n",
    "\n",
    "    optimizer_1.zero_grad()\n",
    "    optimizer_2.zero_grad()\n",
    "\n",
    "\n",
    "    res_A = model(input_A, 'A')\n",
    "    res_B = model(input_B, 'B')\n",
    "\n",
    "    loss1 = criterion(res_A, target_A)\n",
    "    loss2 = criterion(res_B ,target_B)\n",
    "    loss = loss1.item() + loss2.item()\n",
    "    loss1.backward()\n",
    "    loss2.backward()\n",
    "    optimizer_1.step()\n",
    "    optimizer_2.step()\n",
    "    \n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        input_A, target_A = next(test_loader_A)\n",
    "        input_B, target_B = next(test_loader_B)\n",
    "\n",
    "        input_A, target_A = input_A.to(device).float(), target_A.to(device).float()\n",
    "        input_B, target_B = input_B.to(device).float(), target_B.to(device).float()\n",
    "        \n",
    "        res_A = model(input_A, 'A')\n",
    "        res_B = model(input_B, 'B')\n",
    "\n",
    "        loss1 = criterion(res_A, target_A)\n",
    "        loss2 = criterion(res_B ,target_B)\n",
    "        \n",
    "        print('epoch: {}, lossA:{}, lossB:{}'.format(epoch, loss1.item(), loss2.item()))\n",
    "        test_A_ = target_A[0:16]\n",
    "        test_B_ = target_B[0:16]\n",
    "        test_A = target_A[0:16].detach().cpu().numpy().transpose((0,2,3,1))\n",
    "        test_B = target_B[0:16].detach().cpu().numpy().transpose((0,2,3,1))\n",
    "        \n",
    "        print('===> Saving models...')\n",
    "        state = {\n",
    "            'state': model.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/autoencoder.t7')\n",
    "\n",
    "        figure_A = np.stack([\n",
    "            test_A,\n",
    "            model(test_A_, 'A').detach().cpu().numpy().transpose((0,2,3,1)),\n",
    "            model(test_A_, 'B').detach().cpu().numpy().transpose((0,2,3,1)),\n",
    "        ], axis=0)\n",
    "        \n",
    "        figure_B = np.stack([\n",
    "            test_B,\n",
    "            model(test_B_, 'B').detach().cpu().numpy().transpose((0,2,3,1)),\n",
    "            model(test_B_, 'A').detach().cpu().numpy().transpose((0,2,3,1)),\n",
    "        ], axis=0)\n",
    "        figure = np.concatenate([figure_A, figure_B], axis=0)\n",
    "\n",
    "        figure = np.concatenate([it for it in figure],axis = 2)\n",
    "\n",
    "        figure = np.concatenate([it for it in figure],axis = 0)\n",
    "\n",
    "        plt.imsave('train/result.bmp', figure)\n",
    "    \n",
    "#     sheduler_1.step()\n",
    "#     sheduler_2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(test_A_, 'A').detach().cpu().numpy()[0].shape\n",
    "plt.imshow(model(test_A_, 'B').detach().cpu().numpy()[0].transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('video/putin.mp4')\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 1600)\n",
    "_, frame = cap.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "dets = detector(frame, 1)\n",
    "if dets:\n",
    "    left = dets[0].left()\n",
    "    top = dets[0].top()\n",
    "    right = dets[0].right()\n",
    "    bot = dets[0].bottom()\n",
    "\n",
    "croped_face = frame[top:bot, left:right]\n",
    "plt.imshow(croped_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_face = np.expand_dims(cv2.resize(croped_face[int(0.10 * croped_face.shape[0]): int(- 0.10 * croped_face.shape[0]), int(0.10 * croped_face.shape[1]): int(- 0.10 * croped_face.shape[1])], (64,64)).transpose((2,0,1)), axis=0) / 255\n",
    "\n",
    "result = model(torch.tensor(convert_face).to(device).float(), 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(result.detach().cpu().numpy()).transpose((1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
